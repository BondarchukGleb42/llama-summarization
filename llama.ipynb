{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87884723",
   "metadata": {},
   "source": [
    "## Суммаризатор информации о товаре"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eece50c",
   "metadata": {},
   "source": [
    "В этом ноутбуке реализован прототип суммаризатора текстовой информации о товаре на основе модели LLama-7b. Используется зафайнтюненная на задачу суммаризации модель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ce2b8",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45422250",
   "metadata": {},
   "source": [
    "### Импортируем модель .\n",
    "\n",
    "Будем юзать зафайнтюненную на задачу суммаризации ламу. Также, заквантуем её с помощью bits and bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f777ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7844c6998c8247dd9cfa039625c8bf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_id =  \"SalmanFaroz/Llama-2-7b-QLoRa-samsum\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b4ad3",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86cfdd",
   "metadata": {},
   "source": [
    "### Посмотрим на работу модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184478bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Благодаря особому дизайну и использованию инновационных технологий этот чайник готовит чай моментально, сокращая время до минимума.\n",
      "2. Изготовлен из прочных материалов, обеспечивающих долговечность и надежность работы.\n",
      "3. Вмещает до 1,7 литра воды и имеет эргономичную рукоятку для удобной передачи.\n",
      "4. Наличие автоматического выключения и защита от перегрева обеспечивают безопасное использование.\n",
      "5. Электрический чайник \"Quick Boil\" - идеальное решение для быстрых и комфортных заварок чая.\n",
      "\n",
      "1. \"К сожалению, у меня этот чайник сломался через несколько месяцев использования. Заваривать чай больше не получается.\"\n",
      "2. Чайник часто оказывается горячим на поверхности после работы, что делает его использование несколько неудобным.\n",
      "\n",
      "\n",
      "### Рекомендуемый товар:\n",
      "Электрический чайник \"Quick Boil\" - идеальное решение для тех, кто устал ждать заварки чая. Благодаря особому дизайну и использованию инновационных технологий этот чайник готовит чай моментально, сокращая время до минимума. Изготовлен из прочных материалов, обеспечивающих долговечность и надежность работы. Вмещает до 1,7 литра воды и имеет эрго\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Найди плюсы и минусы товара по его текстовому описанию и отзывам.\n",
    "\n",
    "### Товар:\n",
    "текстовое описание: Электрический чайник \"Quick Boil\" - идеальное решение для тех, кто устал ждать заварки чая. Благодаря особому дизайну и использованию инновационных технологий этот чайник готовит чай моментально, сокращая время до минимума. Изготовлен из прочных материалов, обеспечивающих долговечность и надежность работы. Вмещает до 1,7 литра воды и имеет эргономичную рукоятку для удобной передачи. Наличие автоматического выключения и защита от перегрева обеспечивают безопасное использование. Электрический чайник \"Quick Boil\" - идеальное решение для быстрых и комфортных заварок чая.\n",
    "\n",
    "Позитивные отзывы:\n",
    "1. \"Этот чайник настоящий спаситель! Позволяет мне быстро насладиться ароматным чаем в любое время дня. Очень удобно в использовании.\"\n",
    "2. \"Рекомендую этот чайник всем любителям чая! За считанные минуты получаешь горячую воду для заварки чая. Очень стильный и надежный товар.\"\n",
    "\n",
    "Негативные отзывы:\n",
    "1. \"К сожалению, у меня этот чайник сломался через несколько месяцев использования. Заваривать чай больше не получается.\"\n",
    "2. \"Чайник часто оказывается горячим на поверхности после работы, что делает его использование несколько неудобным.\"\n",
    "\n",
    "\n",
    "### Плюсы и минусы:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=400,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(output[len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39d8ef",
   "metadata": {},
   "source": [
    "Модель хорошо понимает требуемую от неё инструкцию. Напишем функцию для получения предиктов и постпроцессинга ответов модели. Заметим, что модель находит плюсы и минусы не только по отзывам, но и в тексте описания товара, сочетая их с отзывами.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0c41ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def predict(dscr, pos_comms, neg_comms):\n",
    "    \"\"\"\n",
    "    Функция для суммаризации информации о товаре и поиску его плюсов и минусов.\n",
    "    Принимает на вход:\n",
    "        -dscr: str, текстовое описание товара;\n",
    "        -pos_comms: list, список позитивных комментариев;\n",
    "        -neg_comms: list, список негативных комментариев.\n",
    "    Генерирует промпт для модели, получает ответ и парсит его. Возвращает dict вида:\n",
    "    {\n",
    "        'pluses': list, список позитивных характеристик товара;\n",
    "        'minuses': list, список негативных характеристик товара.\n",
    "    }\n",
    "    \"\"\"\n",
    "    prompt = \"\\n\".join([\"Найди плюсы и минусы товара по его текстовому описанию и отзывам.\",\n",
    "                      \"### Товар:\",\n",
    "                      dscr,\n",
    "                      \"\\n\"\n",
    "                      \"Позитивные отзывы: \",\n",
    "                      \"\\n\".join([f\"{i+1}) {txt}\" for i, txt in enumerate(pos_comms)]),\n",
    "                      \"\\n\"\n",
    "                      \"Негативные отзывы: \",\n",
    "                      \"\".join([f\"{i+1}) {txt}\" for i, txt in enumerate(neg_comms)]),\n",
    "                      \"### Плюсы и минусы:\"])\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=400,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    output = output[len(prompt):].lower()\n",
    "    if \"плюсы\" in output:\n",
    "        pluses = output[output.index(\"плюсы\")+7:]\n",
    "        if \"минусы\" in output:\n",
    "            pluses = pluses[:pluses.index(\"минусы\")]\n",
    "            pluses = pluses.split(\"\\n\")\n",
    "            pluses = [x[3:] for i, x in enumerate(pluses) if len(x)>0 and str(i+1) in x]\n",
    "        else:\n",
    "            minuses = []\n",
    "    else:\n",
    "        pluses = []\n",
    "            \n",
    "    if \"минусы\" in output:\n",
    "        minuses = output[output.index(\"минусы\")+9:]\n",
    "        minuses = minuses.split(\"\\n\")\n",
    "        minuses = [x[3:] for i, x in enumerate(minuses) if len(x)>0 and str(i+1) in x]\n",
    "    else:\n",
    "        minuses = []\n",
    "    \n",
    "    return {\"pluses\": pluses, \"minuses\": minuses}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7b51e",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e7bf2",
   "metadata": {},
   "source": [
    "### Провалидируем результат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10399e7",
   "metadata": {},
   "source": [
    "Т.к. в общем доступе я не нашёл подходящего к задаче e-commerce датасета на русском, сгенерируем синтетическим датасет с помощью gpt3.5-turbo, и посмотрим, как на нём работает наша модель.  \n",
    "\n",
    "Код для генерации датасета:\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"paste your api key\")\n",
    "\n",
    "dataset = []\n",
    "for _ in tqdm(range(100)):\n",
    "    cont = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Сгенерируй текстовое описание произвольного товара, а также позитивных, 2 негативных и 2 нейтральных отзыва к этому товару. На основе текстового описания этого товара и отзывов сгенерируй краткий список плюсов и минусов для товара. Укажи ответ в формате 'текстовое описание: , позитивные отзывы: , негативные отзывы: , нейтральные отзывы: , плюсы: , минусы: '\",\n",
    "        }],\n",
    "        model=\"gpt-3.5-turbo\").choices[0].message.content\n",
    "    dataset.append(cont)\n",
    "    \n",
    "new_dataset = {\"description\": [], \"positive_comms\": [], \"negative_comms\": [], \"pluses\": [], \"minuses\": []}\n",
    "for x in dataset:\n",
    "    x = x.lower()\n",
    "    try:\n",
    "        dscr = x[x.index(\"текстовое описание:\")+20:x.index(\"позитивные отзывы:\")]\n",
    "        positive = [w for w in x[x.index(\"позитивные отзывы:\")+19:x.index(\"негативные отзывы:\")].split(\"\\n\") if len(w) > 0]\n",
    "        negative = [w for w in x[x.index(\"негативные отзывы:\")+19:x.index(\"нейтральные отзывы: \")].split(\"\\n\") if len(w) > 0]\n",
    "        pluses = x[x.index(\"плюсы\")+7:x.index(\"минусы\")]\n",
    "        minuses = x[x.index(\"минусы\")+8:]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    new_dataset[\"description\"].append(dscr)\n",
    "    new_dataset[\"positive_comms\"].append(positive)\n",
    "    new_dataset[\"negative_comms\"].append(negative)\n",
    "    new_dataset[\"pluses\"].append(pluses)\n",
    "    new_dataset[\"minuses\"].append(minuses)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3238d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "dset = pd.read_csv(\"e-comm.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93afacc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 40/40 [08:44<00:00, 13.11s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for i in tqdm(range(len(dset))):\n",
    "    dscr, pos_comms, neg_comms = dset.iloc[i][\"description\"], dset.iloc[i][\"positive_comms\"], dset.iloc[i][\"negative_comms\"]\n",
    "    outputs.append(predict(dscr, eval(pos_comms), eval(neg_comms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3c098f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pluses': ['1. \"этот чайник позволяет мне быстро насладиться ароматным чаем в любое время дня. очень удобно в использовании.\"2) 2. \"рекомендую этот чайник всем любителям чая! за считанные минуты получаешь горячую воду для заварки чая. очень стильный и надежный товар.\"'],\n",
       " 'minuses': ['1. \"к сожалению, у меня этот чайник сломался через несколько месяцев использования. заваривать чай больше не получается.\"2) 2. \"чайник часто оказывается горячим на поверхности после работы, что делает его использование несколько неудобным.\"']}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc270ac",
   "metadata": {},
   "source": [
    "Модель успешно отработала на 40 тестовых сэмплах, ответы распарсились во всех случаях, где это было возможно. Это говорит о том, что модель хорошо понимает структуру требуемого от неё ответа и редко галлюционирует."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150f0f7",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66de71",
   "metadata": {},
   "source": [
    "### Как можно улучшить результат?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1ccc4",
   "metadata": {},
   "source": [
    "Самый очевидный способ улучшить результат - дофайнтюнить ламу на русскоязычном датасете. Русскоязычный датасет можно также нагенерировать с помощью gpt api.  \n",
    "\n",
    "Но глобально, мне кажется, что задачу можно решить без использования больших LLM: плюсы и минусы в большинстве случаев будут явно указаны либо в тексте описания, либо в тексте отзывов. Поэтому можно обучить отдельные суммаризаторы для поиска плюсов и минусов в тексте товара. Как пример, rubart-cnn или обучить простой linear-attention с помощью метода DSSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08a3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f09bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
